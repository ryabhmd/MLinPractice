# Machine Learning In PracticeGroup members: Raia Abu Ahmad, Thawab AlkhiamiThis is a documentation file for the design decisions that were made during the development of the project.We will explain the decisions made per ML pipeline steps. Namely, justification for why we chose the specific steps that were used in our pipline.# PreprocessingSteps taken in preprocessing:- ``create labels``: Create labels of TRUE (viral) or FALSE (not viral) for each tweet in the dataset, based on the like and retweet counts.                     We did not change the default setting of weights = 1; and threshold = 50.                      - ``punctuation remover``: Remove all punctuation signs from tweets. The rationale is that for further preprocessing steps, there is no need for punctuation to be part of the tweet text,                            as it will not add further meaning that can be used to predict the virality of a tweet. If we think that a certain algorithm benefits from the existence of punctuation,                           we use the raw tweets instead (e.g., in sentiment analysis as will be explained below).                           A few tweaks were done on the original algorithm given to us. Namely, adding the character '±' as a punctuation to remove, as well as removing whitespaces after removing punctuation.                           - ``stop words remover``: Remove stop words from tweets. This is used as a precursor step to tokenization and lemmatization, which are then taken to extract features like TF-IDF and N-grams.                          In addition to this step being a common practice in most NLP pipelines, intuitvely, we don't think that stop words will add any value to the semantic aspect of the tweet,                           which we intend to check in later steps, and this is why we decided to remove them, and focus on content words.                          - ``tokenizer``: Receive the tweets after removing punctuation and stop words, and tokenize them, i.e., segmenti them to separate tokens. This step is done as a precursor to the lemmatization step,                  as the lemmatizer takes as input a list of tokens, rather than a string of tweet text.                 - ``lemmatizer``: Lemmatize each token in each tweet, i.e., replace the token with its lemma/basic dictionary form of a word. We applied lemmatization instead of stemming because we think that lemmatization, though more computationally expensive                  (but in our case it doesn’t seem to take long at all to process), we would like the information of certain verb inflections and nouns to be retained and processed in further steps as the same. E.g., when we work on n-grams, we want ‘high school’ and ‘high schools’ to be counted in the same frequency count.                   We know this can be done using a stemmer, but we have used a lemmatizer because we believe it will yield more specific results in that sense, and conver more cases. E.g., we want 'ran a marathon' and 'run a marathon' to be counted as the same, because thay are exactly the same semantic sense.                   It is important to note, that the unit test showed us that the nltk WordNetLemmatizer is not perfect; e.g. not converting ‘heard’ to ‘hear’ or ‘triggered’ to ‘trigger’. Optimally, we would use another lemmatizer (maybe from spaCy) but for time constraints on this project we kept working with nltk.                   # Feature ExtractionBelow we explain the features we decided to extract, the rationale behind each one of them, and a brief explanation of how they were implemented.- ``sentiment analysis``: It has been proven that sentiment plays a role in the virality of a tweet (e.g. Jenders et al. 2013, 'Analyzing and Predicting Viral Tweets'). Intuitvely as well, we think that whether a tweet is extremely positive or negative plays a role with how people react with it, and thus whether it goes viral.                           We think that mildly positive or mildly negative tweets should be the ones that associate most with virality.                           To implement this, we used nltk's SentimentIntensityAnalyzer. Which, for each sentence returns a dictionary of the following format:                          {'compound': 0.7565, 'neg': 0.092, 'neu': 0.607, 'pos': 0.301}                          Where,'neg', 'neu', and 'pos' give a score of how negative, neutral, and positive a string is, respectively.                          And 'compund' give a score that is set on a scale from -1 (very negative) to +1 (very positive). We use only 'compound' as a sentiment score, because it holds information on all possible sentiments in one number, which would then be easier to give to the classifier.                          The SentimentIntensityAnalyzer was built and trained on social media posts; so giving it the raw tweet is the optional format.                          - ``hashtags numebr``:- ``mentions number``:- ``urls numebr``:- ``tweet length``:- ``time of posting``:- ``asking for retweets``:- ``personal story check``:- ``tweet length``:- ``NER``:- ``TF-IDF``: